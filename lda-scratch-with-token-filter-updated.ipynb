{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA - Scratch implementation with token filtering\n",
    "\n",
    "Here, I'm using LDA(Scratch implementation) model with token filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcOPoLTScu4V"
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're in local machine, you should run this cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're in Google Colab, you should run this cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_PATH = \"<ENTER YOUR DRIVE PATH>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iW6IW_VNDJw"
   },
   "source": [
    "# Load Data\n",
    "Preprocessed training and testing data from \n",
    "[20-news-dataset-pre-processing](https://github.com/nimmitahsin1727/20-news-dataset-pre-processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1AubdN5uGOv"
   },
   "source": [
    "Reading TRAINING from CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "57OiI3tpuF8Y"
   },
   "outputs": [],
   "source": [
    "training_df = pd.read_csv(f'{BASE_PATH}training_df.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebrTGRCCvGsj"
   },
   "source": [
    "Reading TESTING from CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dMD2yGOGvJkK"
   },
   "outputs": [],
   "source": [
    "testing_df = pd.read_csv(f'{BASE_PATH}testing_df.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCWaUQJfKSWq"
   },
   "source": [
    "Create data_words with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bX8K5yK_J6Cw"
   },
   "outputs": [],
   "source": [
    "data_words = training_df.data.map(lambda doc: word_tokenize(doc)).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XPhDJXRWcbl"
   },
   "source": [
    "**Bag of Words on the Data set**\n",
    "\n",
    "Create a dictionary from `data_words` containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbhA7QotULal",
    "outputId": "08a3bae0-b93c-4e8e-b51b-0bcad288aef2"
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  22094\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words: \", len(dictionary.iteritems()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing some samples from dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 actively\n",
      "1 ama\n",
      "2 apr\n",
      "3 assistance\n",
      "4 away\n",
      "5 big\n",
      "6 bike\n",
      "7 board\n",
      "8 camp\n",
      "9 childish\n",
      "10 count\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqPO8s2TWSy-"
   },
   "source": [
    "**Gensim filter_extremes**\n",
    "\n",
    "***Filter out tokens that appear in***\n",
    "\n",
    "less than 15 documents (absolute number) or\n",
    "more than 0.5 documents (fraction of total corpus size, not absolute number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xflYCHdyUggY"
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words after filter:  1832\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words after filter: \", len(dictionary.iteritems()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSbcIn2kWJXm"
   },
   "source": [
    "**Gensim doc2bow**\n",
    "\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NJ8Jp_1aUty_"
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in data_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCIFEKZlWFTk"
   },
   "source": [
    "Preview Bag Of Words for our sample preprocessed document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-UMfDeHNVOUw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 2),\n",
       " (4, 2),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 9),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_doc_0 = bow_corpus[0]\n",
    "bow_doc_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohTYek_MVhHM",
    "outputId": "c59b7432-37ae-4cbb-da8a-31c82769e81c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"ama\") appears 1 time.\n",
      "Word 1 (\"apr\") appears 1 time.\n",
      "Word 2 (\"away\") appears 1 time.\n",
      "Word 3 (\"big\") appears 2 time.\n",
      "Word 4 (\"bike\") appears 2 time.\n",
      "Word 5 (\"board\") appears 1 time.\n",
      "Word 6 (\"count\") appears 1 time.\n",
      "Word 7 (\"dod\") appears 1 time.\n",
      "Word 8 (\"dog\") appears 9 time.\n",
      "Word 9 (\"eat\") appears 1 time.\n",
      "Word 10 (\"face\") appears 1 time.\n",
      "Word 11 (\"great\") appears 1 time.\n",
      "Word 12 (\"hate\") appears 1 time.\n",
      "Word 13 (\"hear\") appears 1 time.\n",
      "Word 14 (\"heard\") appears 1 time.\n",
      "Word 15 (\"large\") appears 1 time.\n",
      "Word 16 (\"leg\") appears 1 time.\n",
      "Word 17 (\"lift\") appears 1 time.\n",
      "Word 18 (\"love\") appears 1 time.\n",
      "Word 19 (\"make\") appears 1 time.\n",
      "Word 20 (\"need\") appears 1 time.\n",
      "Word 21 (\"owner\") appears 1 time.\n",
      "Word 22 (\"party\") appears 1 time.\n",
      "Word 23 (\"personal\") appears 1 time.\n",
      "Word 24 (\"ride\") appears 1 time.\n",
      "Word 25 (\"rider\") appears 1 time.\n",
      "Word 26 (\"say\") appears 1 time.\n",
      "Word 27 (\"seat\") appears 1 time.\n",
      "Word 28 (\"seek\") appears 1 time.\n",
      "Word 29 (\"shit\") appears 1 time.\n",
      "Word 30 (\"shoulder\") appears 1 time.\n",
      "Word 31 (\"thanks\") appears 1 time.\n",
      "Word 32 (\"throw\") appears 1 time.\n",
      "Word 33 (\"work\") appears 1 time.\n",
      "Word 34 (\"yes\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bow_doc_0)):\n",
    "    print(f'Word {bow_doc_0[i][0]} (\\\"{dictionary[bow_doc_0[i][0]]}\\\") appears {bow_doc_0[i][1]} time.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running LDA - Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lda_vb import vbLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832 1832\n"
     ]
    }
   ],
   "source": [
    "n_topic = 10\n",
    "max_iter=100\n",
    "\n",
    "voca = [v for k, v in dictionary.iteritems()]\n",
    "\n",
    "n_doc = len(voca)\n",
    "n_voca = len(dictionary.iteritems())\n",
    "\n",
    "print(n_doc, n_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [list(map(lambda bow: bow[0], bow_doc)) for bow_doc in bow_corpus]\n",
    "doc_cnt = [list(map(lambda bow: bow[1], bow_doc)) for bow_doc in bow_corpus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vb_model = vbLDA(n_doc, n_voca, n_topic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lda_vb_model\u001b[39m.\u001b[39;49mfit(doc_ids, doc_cnt, max_iter\u001b[39m=\u001b[39;49mmax_iter)\n",
      "File \u001b[1;32md:\\university\\concordia\\research\\github\\lda-scratch\\lda_vb.py:46\u001b[0m, in \u001b[0;36mvbLDA.fit\u001b[1;34m(self, doc_ids, doc_cnt, max_iter)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39min\u001b[39;00m xrange(max_iter):\n\u001b[0;32m     45\u001b[0m     tic \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 46\u001b[0m     _, bound \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_m_step(doc_ids, doc_cnt)\n",
      "File \u001b[1;32md:\\university\\concordia\\research\\github\\lda-scratch\\lda_vb.py:97\u001b[0m, in \u001b[0;36mvbLDA.do_m_step\u001b[1;34m(self, doc_ids, doc_cnt)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_m_step\u001b[39m(\u001b[39mself\u001b[39m, doc_ids, doc_cnt):\n\u001b[0;32m     94\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39m    estimate topic distribution based on computed approx. topic distribution\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     (gamma, sstats) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_e_step(doc_ids, doc_cnt)\n\u001b[0;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lambda \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta \u001b[39m+\u001b[39m sstats\n\u001b[0;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Elogbeta \u001b[39m=\u001b[39m dirichlet_expectation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lambda)\n",
      "File \u001b[1;32md:\\university\\concordia\\research\\github\\lda-scratch\\lda_vb.py:61\u001b[0m, in \u001b[0;36mvbLDA.do_e_step\u001b[1;34m(self, doc_ids, doc_cnt)\u001b[0m\n\u001b[0;32m     58\u001b[0m sstats \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lambda\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_doc):\n\u001b[1;32m---> 61\u001b[0m     ids \u001b[39m=\u001b[39m doc_ids[d]\n\u001b[0;32m     62\u001b[0m     cnt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(doc_cnt[d])\n\u001b[0;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39msum(cnt) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lda_vb_model.fit(doc_ids, doc_cnt, max_iter=max_iter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the Keyword in the 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(topic_word_matrix, vocab, topic, n_words=20):\n",
    "    if not isinstance(vocab, np.ndarray):\n",
    "        vocab = np.array(vocab)\n",
    "    top_words = vocab[topic_word_matrix[topic].argsort()[::-1][:n_words]]\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 :  gun,crime,use,firearm,criminal,control,time,like,rate,kill\n",
      "Topic 1 :  file,firearm,state,weapon,law,gun,use,control,handgun,united\n",
      "Topic 2 :  gun,like,cop,police,carry,just,revolver,state,say,behanna\n",
      "Topic 3 :  right,militia,state,government,weapon,law,arm,make,group,just\n",
      "Topic 4 :  image,file,use,jpeg,graphic,format,data,program,software,package\n",
      "Topic 5 :  dod,bike,like,dog,ride,just,say,motorcycle,rid,make\n",
      "Topic 6 :  bike,make,just,good,dod,like,apr,helmet,know,look\n",
      "Topic 7 :  say,fbi,child,atf,compound,day,waco,start,make,come\n",
      "Topic 8 :  file,use,polygon,know,computer,program,look,color,point,need\n",
      "Topic 9 :  know,point,graphic,use,card,need,video,bit,mode,driver\n"
     ]
    }
   ],
   "source": [
    "for ti in range(n_topic):\n",
    "    top_words = get_top_words(lda_vb_model._lambda, voca, ti, n_words=10)\n",
    "    print('Topic', ti ,': ', ','.join(top_words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HS1jG-vbXsWo"
   },
   "source": [
    "**Running LDA - GENSIM**\n",
    "\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DJVJRU3Xl9u",
    "outputId": "a8594686-89cb-45b4-9adc-f154fc840762"
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=n_topic, id2word=dictionary, random_state=100)\n",
    "\n",
    "# lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=n_topic, id2word=dictionary, random_state=100,  passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the Keyword in the 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.014*\"gun\" + 0.012*\"use\" + 0.008*\"point\" + 0.008*\"law\" + 0.007*\"weapon\" + '\n",
      "  '0.007*\"make\" + 0.007*\"like\" + 0.006*\"firearm\" + 0.006*\"right\" + '\n",
      "  '0.005*\"file\"'),\n",
      " (1,\n",
      "  '0.009*\"use\" + 0.009*\"good\" + 0.007*\"time\" + 0.007*\"know\" + 0.007*\"image\" + '\n",
      "  '0.007*\"like\" + 0.006*\"make\" + 0.006*\"bike\" + 0.006*\"graphic\" + '\n",
      "  '0.006*\"just\"'),\n",
      " (2,\n",
      "  '0.008*\"use\" + 0.008*\"gun\" + 0.008*\"like\" + 0.007*\"just\" + 0.006*\"say\" + '\n",
      "  '0.006*\"know\" + 0.006*\"file\" + 0.005*\"time\" + 0.005*\"bike\" + 0.005*\"case\"'),\n",
      " (3,\n",
      "  '0.011*\"use\" + 0.010*\"image\" + 0.006*\"know\" + 0.006*\"say\" + 0.005*\"dod\" + '\n",
      "  '0.005*\"file\" + 0.005*\"data\" + 0.005*\"just\" + 0.005*\"dog\" + 0.005*\"like\"'),\n",
      " (4,\n",
      "  '0.011*\"like\" + 0.010*\"know\" + 0.009*\"make\" + 0.009*\"thing\" + 0.009*\"say\" + '\n",
      "  '0.008*\"gun\" + 0.007*\"good\" + 0.007*\"bike\" + 0.007*\"just\" + 0.007*\"apr\"'),\n",
      " (5,\n",
      "  '0.011*\"use\" + 0.007*\"just\" + 0.007*\"image\" + 0.007*\"time\" + 0.006*\"gun\" + '\n",
      "  '0.006*\"bike\" + 0.006*\"like\" + 0.005*\"graphic\" + 0.005*\"good\" + '\n",
      "  '0.005*\"know\"'),\n",
      " (6,\n",
      "  '0.010*\"like\" + 0.010*\"use\" + 0.008*\"say\" + 0.008*\"know\" + 0.007*\"need\" + '\n",
      "  '0.007*\"computer\" + 0.007*\"gun\" + 0.007*\"just\" + 0.007*\"make\" + '\n",
      "  '0.006*\"bike\"'),\n",
      " (7,\n",
      "  '0.014*\"gun\" + 0.010*\"right\" + 0.010*\"make\" + 0.010*\"state\" + 0.009*\"use\" + '\n",
      "  '0.007*\"time\" + 0.007*\"good\" + 0.006*\"just\" + 0.005*\"militia\" + '\n",
      "  '0.005*\"weapon\"'),\n",
      " (8,\n",
      "  '0.025*\"file\" + 0.013*\"image\" + 0.009*\"use\" + 0.007*\"know\" + 0.006*\"jpeg\" + '\n",
      "  '0.006*\"like\" + 0.006*\"say\" + 0.006*\"apr\" + 0.006*\"format\" + 0.005*\"make\"'),\n",
      " (9,\n",
      "  '0.015*\"gun\" + 0.008*\"use\" + 0.007*\"right\" + 0.007*\"bike\" + 0.007*\"make\" + '\n",
      "  '0.007*\"like\" + 0.007*\"know\" + 0.006*\"say\" + 0.006*\"state\" + 0.006*\"just\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UcOPoLTScu4V",
    "S_BBNjjzc4m5",
    "c0cAeBowGUVP"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lda-implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1669881b8e0ee381f1d44208a6e6b4675430ed382f288976bd9acdbb8db18405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
